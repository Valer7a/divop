{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import pandas as pd\n",
    "import divop\n",
    "#import notebooks.divop as divop\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import imp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_synthetic_data(data,path_positives,path_negatives, n_iterations,n_positives,n_negatives,frac_positives=None,frac_negatives=None):\n",
    "    #load the previous created synthetic training data\n",
    "    test_data_positives=pd.read_csv(path_positives, index_col=0) #positives\n",
    "    test_data_negatives= pd.read_csv(path_negatives, index_col=0) #negatives\n",
    "    positives=test_data_positives[test_data_positives[\"Type\"]=='g'].sample(n=n_positives,frac=frac_positives) \n",
    "    negatives=test_data_negatives[test_data_negatives[\"Type\"]=='b'].sample(n=n_negatives,frac=frac_negatives)\n",
    "    #define the dataframe containing the validation metrics (1 row for each synthetic dataset)\n",
    "    metrics = pd.DataFrame(columns=['f1','precision','recall','auc'])\n",
    "    for i in range(n_iterations):\n",
    "        print(i)\n",
    "        #creating the sequences\n",
    "        df_synthetic_test = divop.collect_synthetic_data(data, iterations=1, n=4)#to create 1 dataset of synthetic data\n",
    "        imp.reload(divop) #to empty the dataframes in the library\n",
    "        #classifying the paths in the test set and calculating the validation metrics\n",
    "        print('classifying synthetic data')\n",
    "        classified_data, metrics_list = divop.classify_higher_order_paths(df_synthetic_test,n_negatives=negatives, n_positives=positives, synthetic=True, var_order=False)\n",
    "        metrics.loc[i] = metrics_list #adding metrics results to the metrics df\n",
    "        classified_data['cycle'] = i # we create a column containing, for each row, in which cycle it has been generated, to eventually evaluate validation metric later\n",
    "        if i==0:\n",
    "            classified_data_previous = classified_data\n",
    "        elif i==(n_iterations-1):\n",
    "            classified_data_all = pd.concat([classified_data_previous, classified_data],ignore_index=True)\n",
    "            print('FINAL VALIDATION METRICS EVALUATION')\n",
    "            print('f1 score: '+ str(metrics['f1'].mean()) + '+/-' + str(metrics['f1'].std()))\n",
    "            print('precision score: '+ str(metrics['precision'].mean()) + '+/-' + str(metrics['precision'].std()))\n",
    "            print('recall score: '+ str(metrics['recall'].mean()) + '+/-' + str(metrics['recall'].std()))\n",
    "            print('auc score: '+ str(metrics['auc'].mean()) + '+/-' + str(metrics['auc'].std()))\n",
    "        else:\n",
    "            classified_data_all = pd.concat([classified_data_previous, classified_data],ignore_index=True)\n",
    "            classified_data_previous = classified_data_all\n",
    "        \n",
    "    return classified_data_all,metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flight = []\n",
    "path_data = \"../data/input/coupons_2018_01-5percent.ngram\"\n",
    "with open(path_data) as f:\n",
    "    for line in f:\n",
    "        line = line.split(\",\")\n",
    "        chain = [str(x) for x in line[:-1]]\n",
    "        data_flight.append(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_positives = \"../data/input/flight_positives_training_data.csv\"\n",
    "path_negatives = \"../data/input/flight_negatives_training_data.csv.xz\"\n",
    "n_iterations=3\n",
    "n_positives = None\n",
    "n_negatives = None\n",
    "#Here we test flight data training the algorithm through synthetic flight data\n",
    "flight_testing_df, metrics = testing_synthetic_data(data_flight, path_positives, path_negatives, n_iterations,n_positives,n_negatives,frac_positives=1,frac_negatives=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing flight data, after training the algorithm through orbis synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_positives = \"../data/input/orbis_positives_training_data.csv.csv\"\n",
    "path_negatives = \"../data/input/orbis_negatives_training_data.csv.xz\"\n",
    "n_iterations=3\n",
    "n_positives = None\n",
    "n_negatives = None\n",
    "#NOTE!: here we are testing flight data training the algorithm through synthetic Orbis data. Even though we still obtain good results, it is not the tested and best way to go\n",
    "#The correct way is to create training flight data and train the algorithm through it, however, you can test your data in this way if you want a sight of the performance thorugh this method\n",
    "flight_testing_df, metrics = testing_synthetic_data(data_flight, path_positives, path_negatives, n_iterations,n_positives,n_negatives,frac_positives=1,frac_negatives=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
